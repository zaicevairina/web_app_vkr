{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/kirill/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/kirill/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/kirill/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/kirill/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader\n",
    "from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator\n",
    "from deeppavlov.models.preprocessors.str_lower import str_lower\n",
    "from deeppavlov.models.tokenizers.nltk_moses_tokenizer import NLTKMosesTokenizer\n",
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "from deeppavlov.models.sklearn import SklearnComponent\n",
    "from deeppavlov.metrics.accuracy import sets_accuracy\n",
    "import numpy as np\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "stops.add('рис')\n",
    "stops.add('университет')\n",
    "stops.add('брянск')\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "dict_stop=set(['метод','определение','условие','момент','значение','результат','критерий',\n",
    "               'работа','вариант','брянский государственный университет','научнотехнический вестник',\n",
    "              'соответствие','такой образ','весь критерий','пример','выбор','ключевое слово','период',\n",
    "              'уравнение','формула','множитель','повышение','оценка','проведение',\n",
    "              'машина','нагрузка','брянская область','точка','случай','расчет','таблица','расчёт',\n",
    "              'с показатель','град','обработка','статья','элемент','раз','применение','центр','форма'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_text(review):\n",
    "    review_text = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    words = [w for w in words if not w in stops]\n",
    "    words = [morph.parse(w)[0].normal_form for w in words]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание dataset/x-статья,y-tag(0,1)\n",
    "# dataset = [sentence_origin,sentence_tr,tag,kws,[kw_in_sentence]]\n",
    "dataset = []\n",
    "files=['./articles_2019/articles.csv','./articles_2018/articles.csv','./articles_2017/articles.csv']\n",
    "for file in files:\n",
    "    with open(file,'r', encoding='utf-8',newline='') as f:\n",
    "        reader = csv.reader(f,delimiter=',')\n",
    "        for row in reader:\n",
    "            \n",
    "            if row!=[]:\n",
    "#                 print('eeest')\n",
    "                kws=''.join(row[0])\n",
    "                text = ''.join(row[2])\n",
    "                sentences_origin = text.split('.')\n",
    "                sentences_tr = list(map(treatment_text,sentences_origin))\n",
    "                kws_tr = treatment_text(kws)  \n",
    "                kws_tr_set=set(kws_tr)\n",
    "                \n",
    "                for sentence_tr,sentence_or in zip(sentences_tr,sentences_origin):\n",
    "                    sentence_tr_set=set(sentence_tr)\n",
    "                    if len(sentence_tr)>5 and kws_tr_set.intersection(sentence_tr_set):\n",
    "                        dataset.append((sentence_or,' '.join(sentence_tr),'1',kws,' '.join(kws_tr_set.intersection(sentence_tr_set))))\n",
    "                    elif len(sentence_tr)>5:\n",
    "                        dataset.append((sentence_or,' '.join(sentence_tr),'0',kws,'None'))\n",
    "#             else:\n",
    "#                 print('pusto')\n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-01 23:59:56.573 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 109: Cannot find valid.csv file\n",
      "2020-03-01 23:59:56.574 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 109: Cannot find test.csv file\n"
     ]
    }
   ],
   "source": [
    "# прочитать dataset из csv файла\n",
    "dr = BasicClassificationDatasetReader().read(\n",
    "    data_path='./',\n",
    "    train='dataset.csv',\n",
    "    x = 'text',\n",
    "    y = 'tag',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-01 23:59:57.881 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 74: Splitting field <<train>> to new fields <<['train', 'valid']>>\n"
     ]
    }
   ],
   "source": [
    "# initialize data iterator splitting `train` field to `train` and `valid` in proportion 0.8/0.2\n",
    "train_iterator = BasicClassificationDatasetIterator(\n",
    "    data=dr,\n",
    "    field_to_split='train',  # field that will be splitted\n",
    "    split_fields=['train', 'valid'],   # fields to which the fiald above will be splitted\n",
    "    split_proportions=[0.8, 0.2],  #proportions for splitting\n",
    "    split_seed=23,  # seed for splitting dataset 23\n",
    "    seed=42)  # seed for iteration over dataset 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = NLTKMosesTokenizer()\n",
    "train_x_lower_tokenized = str_lower(tokenizer(train_iterator.get_instances(data_type='train')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-01 23:59:59.992 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 45: Load path '/home/kirill/2019-2-Atom-Backend-K-Kondratenya/key_words/classes.dict' differs from save path '/home/kirill/2019-2-Atom-Backend-K-Kondratenya/key_words/.classes.dict' in 'infer' mode for SimpleVocabulary.\n",
      "2020-03-02 00:00:00.71 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /home/kirill/2019-2-Atom-Backend-K-Kondratenya/key_words/.classes.dict]\n"
     ]
    }
   ],
   "source": [
    "# initialize simple vocabulary to collect all appeared in the dataset classes\n",
    "classes_vocab = SimpleVocabulary(\n",
    "    save_path='.classes.dict',\n",
    "    load_path='./classes.dict')\n",
    "classes_vocab.fit((train_iterator.get_instances(data_type='train')[1]))\n",
    "classes_vocab.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-02 00:00:00.495 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 101: [saving vocabulary to /home/kirill/2019-2-Atom-Backend-K-Kondratenya/key_words/tokens.dict]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', 2477),\n",
       " ('2', 1728),\n",
       " ('0', 1702),\n",
       " ('3', 1380),\n",
       " ('4', 1163),\n",
       " ('5', 951),\n",
       " ('элемент', 937),\n",
       " ('систем', 928),\n",
       " ('государствен', 863),\n",
       " ('6', 822)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also one can collect vocabulary of textual tokens appeared 2 and more times in the dataset\n",
    "token_vocab = SimpleVocabulary(\n",
    "    save_path='./tokens.dict',\n",
    "    load_path='./tokens.dict',\n",
    "    min_freq=2,\n",
    "    special_tokens=('<PAD>', '<UNK>',),\n",
    "    unk_token='<UNK>')\n",
    "token_vocab.fit(train_x_lower_tokenized)\n",
    "token_vocab.save()\n",
    "token_vocab.freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-02 00:00:01.96 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 219: Cannot load model from /home/kirill/2019-2-Atom-Backend-K-Kondratenya/key_words/tfidf_v0.pkl\n",
      "2020-03-02 00:00:01.96 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 166: Initializing model sklearn.feature_extraction.text:TfidfVectorizer from scratch\n",
      "2020-03-02 00:00:01.304 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.text:TfidfVectorizer\n",
      "2020-03-02 00:00:01.868 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/kirill/2019-2-Atom-Backend-K-Kondratenya/key_words/tfidf_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "# initialize TF-IDF vectorizer sklearn component with `transform` as infer method\n",
    "tfidf = SklearnComponent(\n",
    "    model_class=\"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "    infer_method=\"transform\",\n",
    "    save_path='./tfidf_v0.pkl',\n",
    "    load_path='./tfidf_v0.pkl',\n",
    "    mode='train')\n",
    "tfidf.fit(str_lower(train_iterator.get_instances(data_type='train')[0]))\n",
    "tfidf.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all train and valid data from iterator\n",
    "x_train, y_train = train_iterator.get_instances(data_type=\"train\")\n",
    "x_valid, y_valid = train_iterator.get_instances(data_type=\"valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-02 00:00:03.539 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 219: Cannot load model from /home/kirill/2019-2-Atom-Backend-K-Kondratenya/key_words/logreg_v0.pkl\n",
      "2020-03-02 00:00:03.540 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 166: Initializing model sklearn.linear_model:LogisticRegression from scratch\n"
     ]
    }
   ],
   "source": [
    "# initialize sklearn classifier, all parameters for classifier could be passed\n",
    "cls = SklearnComponent(\n",
    "    model_class=\"sklearn.linear_model:LogisticRegression\",\n",
    "    infer_method=\"predict\",\n",
    "    save_path='./logreg_v0.pkl',\n",
    "    load_path='./logreg_v0.pkl',\n",
    "    C=1,\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-02 00:00:05.524 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.linear_model:LogisticRegression\n",
      "/home/kirill/jupiter_dir/jupiter_venv/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2020-03-02 00:00:05.643 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to /home/kirill/2019-2-Atom-Backend-K-Kondratenya/key_words/logreg_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "# fit sklearn classifier and save it\n",
    "cls.fit(tfidf(x_train), y_train)\n",
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cls_model.pickle', 'wb') as f:\n",
    "    pickle.dump(cls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tf_idf_model.pickle', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
