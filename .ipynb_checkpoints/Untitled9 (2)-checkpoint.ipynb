{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader\n",
    "from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator\n",
    "# from deeppavlov.models.preprocessors.str_lower import str_lower\n",
    "from deeppavlov.models.tokenizers.nltk_moses_tokenizer import NLTKMosesTokenizer\n",
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "from deeppavlov.models.sklearn import SklearnComponent\n",
    "from deeppavlov.metrics.accuracy import sets_accuracy\n",
    "import numpy as np\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "stops.add('рис')\n",
    "stops.add('университет')\n",
    "stops.add('брянск')\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "dict_stop=set(['метод','определение','условие','момент','значение','результат','критерий',\n",
    "               'работа','вариант','брянский государственный университет','научнотехнический вестник',\n",
    "              'соответствие','такой образ','весь критерий','пример','выбор','ключевое слово','период',\n",
    "              'уравнение','формула','множитель','повышение','оценка','проведение',\n",
    "              'машина','нагрузка','брянская область','точка','случай','расчет','таблица','расчёт',\n",
    "              'с показатель','град','обработка','статья','элемент','раз','применение','центр','форма'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_text(review):\n",
    "    review_text = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    words = [w for w in words if not w in stops]\n",
    "    words = [morph.parse(w)[0].normal_form for w in words]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание dataset/x-статья,y-tag(0,1)\n",
    "# dataset = [sentence_origin,sentence_tr,tag,kws,[kw_in_sentence]]\n",
    "dataset = []\n",
    "k = 'C:/Users/Ирина/PycharmProjects/UIR7sem/venv'\n",
    "files=[k+'/articles_2019/articles.csv',k+'/articles_2018/articles.csv',k+'/articles_2017/articles.csv']\n",
    "for file in files:\n",
    "    with open(file,'r', encoding='utf-8',newline='') as f:\n",
    "        reader = csv.reader(f,delimiter=',')\n",
    "        for row in reader:\n",
    "            \n",
    "            if row!=[]:\n",
    "#                 print('eeest')\n",
    "                kws=''.join(row[0])\n",
    "                text = ''.join(row[2])\n",
    "                sentences_origin = text.split('.')\n",
    "                sentences_tr = list(map(treatment_text,sentences_origin))\n",
    "                kws_tr = treatment_text(kws)  \n",
    "                kws_tr_set=set(kws_tr)\n",
    "                \n",
    "                for sentence_tr,sentence_or in zip(sentences_tr,sentences_origin):\n",
    "                    sentence_tr_set=set(sentence_tr)\n",
    "                    if len(sentence_tr)>5 and kws_tr_set.intersection(sentence_tr_set):\n",
    "                        dataset.append((sentence_or,' '.join(sentence_tr),'1',kws,' '.join(kws_tr_set.intersection(sentence_tr_set))))\n",
    "                    elif len(sentence_tr)>5:\n",
    "                        dataset.append((sentence_or,' '.join(sentence_tr),'0',kws,'None'))\n",
    "#             else:\n",
    "#                 print('pusto')\n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-06 22:41:39.426 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 95: Cannot find C:\\Users\\Ирина\\PycharmProjects\\UIR7sem\\venv\\valid.csv file\n"
     ]
    }
   ],
   "source": [
    "# прочитать dataset из csv файла\n",
    "dr = BasicClassificationDatasetReader().read(\n",
    "    data_path=k,\n",
    "    train='dataset.csv',\n",
    "    x = 'text',\n",
    "    y = 'tag',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-06 22:41:44.877 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 73: Splitting field <<train>> to new fields <<['train', 'valid']>>\n"
     ]
    }
   ],
   "source": [
    "# initialize data iterator splitting `train` field to `train` and `valid` in proportion 0.8/0.2\n",
    "train_iterator = BasicClassificationDatasetIterator(\n",
    "    data=dr,\n",
    "    field_to_split='train',  # field that will be splitted\n",
    "    split_fields=['train', 'valid'],   # fields to which the fiald above will be splitted\n",
    "    split_proportions=[0.8, 0.2],  #proportions for splitting\n",
    "    split_seed=23,  # seed for splitting dataset 23\n",
    "    seed=42)  # seed for iteration over dataset 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = NLTKMosesTokenizer()\n",
    "train_x_lower_tokenized = tokenizer(train_iterator.get_instances(data_type='train')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-06 22:42:06.499 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 43: Load path 'classes.dict' differs from save path 'C:\\Users\\Ирина\\Desktop\\keywords\\.classes.dict' in 'infer' mode for SimpleVocabulary.\n",
      "2020-05-06 22:42:06.603 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 98: [saving vocabulary to C:\\Users\\Ирина\\Desktop\\keywords\\.classes.dict]\n"
     ]
    }
   ],
   "source": [
    "# initialize simple vocabulary to collect all appeared in the dataset classes\n",
    "classes_vocab = SimpleVocabulary(\n",
    "    save_path='.classes.dict',\n",
    "    load_path='./classes.dict')\n",
    "classes_vocab.fit((train_iterator.get_instances(data_type='train')[1]))\n",
    "classes_vocab.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-06 22:42:06.695 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 112: [loading vocabulary from C:\\Users\\Ирина\\Desktop\\keywords\\tokens.dict]\n",
      "2020-05-06 22:42:07.687 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 98: [saving vocabulary to C:\\Users\\Ирина\\Desktop\\keywords\\tokens.dict]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('1', 2477),\n",
       " ('2', 1728),\n",
       " ('0', 1702),\n",
       " ('3', 1380),\n",
       " ('4', 1163),\n",
       " ('5', 951),\n",
       " ('элемент', 937),\n",
       " ('систем', 928),\n",
       " ('государствен', 863),\n",
       " ('6', 822)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also one can collect vocabulary of textual tokens appeared 2 and more times in the dataset\n",
    "token_vocab = SimpleVocabulary(\n",
    "    save_path='./tokens.dict',\n",
    "    load_path='./tokens.dict',\n",
    "    min_freq=2,\n",
    "    special_tokens=('<PAD>', '<UNK>',),\n",
    "    unk_token='<UNK>')\n",
    "token_vocab.fit(train_x_lower_tokenized)\n",
    "token_vocab.save()\n",
    "token_vocab.freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-06 22:42:24.134 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 202: Loading model sklearn.feature_extraction.text:TfidfVectorizer from C:\\Users\\Ирина\\Desktop\\keywords\\tfidf_v0.pkl\n",
      "2020-05-06 22:42:24.190 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 209: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-05-06 22:42:24.198 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 215: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2020-05-06 22:42:24.574 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 108: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-05-06 22:42:26.294 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 240: Saving model to C:\\Users\\Ирина\\Desktop\\keywords\\tfidf_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "# initialize TF-IDF vectorizer sklearn component with `transform` as infer method\n",
    "tfidf = SklearnComponent(\n",
    "    model_class=\"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "    infer_method=\"transform\",\n",
    "    save_path='./tfidf_v0.pkl',\n",
    "    load_path='./tfidf_v0.pkl',\n",
    "    mode='train')\n",
    "tfidf.fit(train_iterator.get_instances(data_type='train')[0])\n",
    "tfidf.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all train and valid data from iterator\n",
    "x_train, y_train = train_iterator.get_instances(data_type=\"train\")\n",
    "x_valid, y_valid = train_iterator.get_instances(data_type=\"valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-06 22:46:19.883 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 202: Loading model sklearn.linear_model:LogisticRegression from C:\\Users\\Ирина\\Desktop\\keywords\\logreg_v0.pkl\n",
      "2020-05-06 22:46:19.895 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 209: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2020-05-06 22:46:19.903 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 215: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "# initialize sklearn classifier, all parameters for classifier could be passed\n",
    "cls = SklearnComponent(\n",
    "    model_class=\"sklearn.linear_model:LogisticRegression\",\n",
    "    infer_method=\"predict\",\n",
    "    save_path='./logreg_v0.pkl',\n",
    "    load_path='./logreg_v0.pkl',\n",
    "    C=1,\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-06 22:46:24.375 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 108: Fitting model sklearn.linear_model.logisticLogisticRegression\n",
      "C:\\python\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "2020-05-06 22:46:24.603 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 240: Saving model to C:\\Users\\Ирина\\Desktop\\keywords\\logreg_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "# fit sklearn classifier and save it\n",
    "cls.fit(tfidf(x_train), y_train)\n",
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('model.pickle', 'wb') as f:\n",
    "#     pickle.dump(cls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('tfidf.pickle', 'wb') as f:\n",
    "#     pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=cls(tfidf(x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.87      0.86      1362\n",
      "           1       0.84      0.82      0.83      1172\n",
      "\n",
      "    accuracy                           0.85      2534\n",
      "   macro avg       0.85      0.84      0.84      2534\n",
      "weighted avg       0.85      0.85      0.85      2534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_valid = list(map(lambda x: int(x[0]),y_valid))\n",
    "pred = list(map(lambda x: int(x[0]),pred))\n",
    "print(classification_report(y_valid, pred, labels=[0, 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "project_id = 'arctic-task-238719'\n",
    "private_key='arctic-task-238719-e6a1c5fe056b.json'\n",
    "from google.cloud import bigquery\n",
    "credentials = service_account.Credentials.from_service_account_file('./arctic-task-238719-e6a1c5fe056b.json')\n",
    "from pandas.io import gbq\n",
    "import pickle\n",
    "import re\n",
    "from deeppavlov.models.sklearn import SklearnComponent\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "import pickle\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "stops.add('рис')\n",
    "stops.add('университет')\n",
    "stops.add('брянск')\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "def treatment_text(review):\n",
    "    review_text = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    words = [w for w in words if not w in stops]\n",
    "    words = [morph.parse(w)[0].normal_form for w in words]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    words = [w for w in words if not w in stops]\n",
    "    return(' '.join(words))\n",
    "\n",
    "\n",
    "with open('dataset.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# print(df['keywords'] .head(100))\n",
    "\n",
    "\n",
    "# initialize TF-IDF vectorizer sklearn component with `transform` as infer method\n",
    "# tfidf_knn = SklearnComponent(\n",
    "#     model_class=\"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "#     infer_method=\"transform\",\n",
    "#     save_path='./tfidf_knn.pkl',\n",
    "#     load_path='./tfidf_knn.pkl',\n",
    "#     mode='train')\n",
    "#\n",
    "x = df['keywords'][:-1].values.tolist()\n",
    "x = tuple(x)\n",
    "# tfidf_knn.fit(x)\n",
    "# tfidf_knn.save()\n",
    "# with open('tfidf_knn.pickle', 'wb') as f:\n",
    "#     pickle.dump(tfidf_knn, f)\n",
    "\n",
    "with open('tfidf_knn.pickle', 'rb') as f:\n",
    "    tfidf_knn = pickle.load(f)\n",
    "y = tfidf_knn(('физика частицы квант взрыв',))\n",
    "print(y)\n",
    "#\n",
    "# # from sklearn.neighbors import NearestNeighbors\n",
    "# # x_pre =tfidf_knn(x)\n",
    "#\n",
    "# # neigh = NearestNeighbors(n_neighbors=5).fit(x_pre)\n",
    "# x_pre = x_pre.toarray()\n",
    "# if x_pre[0].all()==x_pre[1000].all():\n",
    "#     print(True)\n",
    "# answer=neigh.kneighbors(y)\n",
    "# print(neigh.kneighbors(y))\n",
    "# print(len(x_pre))\n",
    "# print('x_pre[i]',x_pre[10000])\n",
    "#\n",
    "# for i in range(x_pre.shape[0]):\n",
    "#     if x_pre[i]==answer[0]:\n",
    "#         print(x[i])\n",
    "# # print(tfidf_knn.inverse_transform(y))\n",
    "\n",
    "#\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "#\n",
    "# count_vectorizer = CountVectorizer()\n",
    "# #Apply this vectorizer to text to get a sparse matrix of counts\n",
    "# count_matrix = count_vectorizer.fit_transform(df['keywords'])\n",
    "# #Get the names of the features\n",
    "# features = count_vectorizer.get_feature_names()\n",
    "# #Create a series from the sparse matrix\n",
    "# d = pd.Series(count_matrix.toarray().flatten(),index = features).sort_values(ascending=False)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from deeppavlov.models.sklearn import SklearnComponent\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "stops.add('рис')\n",
    "stops.add('университет')\n",
    "stops.add('брянск')\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "def treatment_text(review):\n",
    "    try:\n",
    "        review_text = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", review)\n",
    "        words = review_text.lower().split()\n",
    "        words = [w for w in words if not w in stops]\n",
    "        words = [morph.parse(w)[0].normal_form for w in words]\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        words = [w for w in words if not w in stops]\n",
    "        return(' '.join(words))\n",
    "    except:\n",
    "        return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('dataset.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "df['title_tr'] = df['title'].apply(treatment_text)\n",
    "# df['title_kws']=df['title'].apply(treatment_text)+df['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>title_tr</th>\n",
       "      <th>title_kws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Калачев Глеб Вячеславович</td>\n",
       "      <td>О мощностной сложности плоских схем</td>\n",
       "      <td>дискретн математик математическ кибернетик выч...</td>\n",
       "      <td>мощностн сложност плоск схем</td>\n",
       "      <td>мощностн сложност плоск схемдискретн математик...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Лутцева Елена Андреевна</td>\n",
       "      <td>Педагогические основы взаимосвязи урочной и вн...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>педагогическ основ взаимосвяз урочн внеурочн т...</td>\n",
       "      <td>педагогическ основ взаимосвяз урочн внеурочн т...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Малинин Алексей Николаевич</td>\n",
       "      <td>Релятивистские идеи в курсе теоретической физи...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>релятивистск иде курс теоретическ физик педвуз</td>\n",
       "      <td>релятивистск иде курс теоретическ физик педвуз...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Царева Светлана Евгеньевна</td>\n",
       "      <td>Формирование учебной деятельности младших школ...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>формирован учебн деятельн младш школьник обуче...</td>\n",
       "      <td>формирован учебн деятельн младш школьник обуче...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Лебедко Валерий Константинович</td>\n",
       "      <td>Формирование пространственных представлений на...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>формирован пространствен представлен занят рис...</td>\n",
       "      <td>формирован пространствен представлен занят рис...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Подольский Александр Иванович</td>\n",
       "      <td>Организация учебной деятельности школьников пр...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>организац учебн деятельн школьник формирован п...</td>\n",
       "      <td>организац учебн деятельн школьник формирован п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Крахоткина Валентина Кузьминична</td>\n",
       "      <td>Учебно-исследовательская работа студентов по м...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>учебн исследовательск работ студент методик пр...</td>\n",
       "      <td>учебн исследовательск работ студент методик пр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Никифорова Валентина Михайловна</td>\n",
       "      <td>Совершенствование преподавания электрорадиотех...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>совершенствован преподаван электрорадиотехник ...</td>\n",
       "      <td>совершенствован преподаван электрорадиотехник ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Абдукаримов Мамадали</td>\n",
       "      <td>Формирование логических приемов мышления у уча...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>формирован логическ мышлен уча 6 8 класс обуче...</td>\n",
       "      <td>формирован логическ мышлен уча 6 8 класс обуче...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ильигорский Юрий Константинович</td>\n",
       "      <td>Особенности организации учебного эксперимента ...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>особен организац учебн эксперимент школ углубл...</td>\n",
       "      <td>особен организац учебн эксперимент школ углубл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Икрамов Джурабай</td>\n",
       "      <td>Развитие математической культуры школьников  Я...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>развит математическ культур школьник языков ас...</td>\n",
       "      <td>развит математическ культур школьник языков ас...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Тростенцова Лидия Александровна</td>\n",
       "      <td>Обучение морфологии на основе ее обобщенных по...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>обучен морфолог основ обобщен понят систематич...</td>\n",
       "      <td>обучен морфолог основ обобщен понят систематич...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Разумовская Маргарита Михайловна</td>\n",
       "      <td>Теоретические основы обучения орфографии в сре...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>теоретическ основ обучен орфограф средн школ</td>\n",
       "      <td>теоретическ основ обучен орфограф средн школме...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Оганесян Сергей Саядович</td>\n",
       "      <td>Лингводидактические основы создания и использо...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>лингводидактическ основ создан использован ауд...</td>\n",
       "      <td>лингводидактическ основ создан использован ауд...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Мурина Лариса Александровна</td>\n",
       "      <td>Методические основы обучения культуре русской ...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>методическ основ обучен культур русск реч школ...</td>\n",
       "      <td>методическ основ обучен культур русск реч школ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              author  \\\n",
       "0          Калачев Глеб Вячеславович   \n",
       "1            Лутцева Елена Андреевна   \n",
       "2         Малинин Алексей Николаевич   \n",
       "3         Царева Светлана Евгеньевна   \n",
       "4     Лебедко Валерий Константинович   \n",
       "5      Подольский Александр Иванович   \n",
       "6   Крахоткина Валентина Кузьминична   \n",
       "7    Никифорова Валентина Михайловна   \n",
       "8               Абдукаримов Мамадали   \n",
       "9    Ильигорский Юрий Константинович   \n",
       "10                  Икрамов Джурабай   \n",
       "11   Тростенцова Лидия Александровна   \n",
       "12  Разумовская Маргарита Михайловна   \n",
       "13          Оганесян Сергей Саядович   \n",
       "14       Мурина Лариса Александровна   \n",
       "\n",
       "                                                title  \\\n",
       "0                О мощностной сложности плоских схем    \n",
       "1   Педагогические основы взаимосвязи урочной и вн...   \n",
       "2   Релятивистские идеи в курсе теоретической физи...   \n",
       "3   Формирование учебной деятельности младших школ...   \n",
       "4   Формирование пространственных представлений на...   \n",
       "5   Организация учебной деятельности школьников пр...   \n",
       "6   Учебно-исследовательская работа студентов по м...   \n",
       "7   Совершенствование преподавания электрорадиотех...   \n",
       "8   Формирование логических приемов мышления у уча...   \n",
       "9   Особенности организации учебного эксперимента ...   \n",
       "10  Развитие математической культуры школьников  Я...   \n",
       "11  Обучение морфологии на основе ее обобщенных по...   \n",
       "12  Теоретические основы обучения орфографии в сре...   \n",
       "13  Лингводидактические основы создания и использо...   \n",
       "14  Методические основы обучения культуре русской ...   \n",
       "\n",
       "                                             keywords  \\\n",
       "0   дискретн математик математическ кибернетик выч...   \n",
       "1   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "2   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "3   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "4   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "5   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "6   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "7   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "8   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "9   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "10  методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "11  методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "12  методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "13  методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "14  методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "\n",
       "                                             title_tr  \\\n",
       "0                        мощностн сложност плоск схем   \n",
       "1   педагогическ основ взаимосвяз урочн внеурочн т...   \n",
       "2      релятивистск иде курс теоретическ физик педвуз   \n",
       "3   формирован учебн деятельн младш школьник обуче...   \n",
       "4   формирован пространствен представлен занят рис...   \n",
       "5   организац учебн деятельн школьник формирован п...   \n",
       "6   учебн исследовательск работ студент методик пр...   \n",
       "7   совершенствован преподаван электрорадиотехник ...   \n",
       "8   формирован логическ мышлен уча 6 8 класс обуче...   \n",
       "9   особен организац учебн эксперимент школ углубл...   \n",
       "10  развит математическ культур школьник языков ас...   \n",
       "11  обучен морфолог основ обобщен понят систематич...   \n",
       "12       теоретическ основ обучен орфограф средн школ   \n",
       "13  лингводидактическ основ создан использован ауд...   \n",
       "14  методическ основ обучен культур русск реч школ...   \n",
       "\n",
       "                                            title_kws  \n",
       "0   мощностн сложност плоск схемдискретн математик...  \n",
       "1   педагогическ основ взаимосвяз урочн внеурочн т...  \n",
       "2   релятивистск иде курс теоретическ физик педвуз...  \n",
       "3   формирован учебн деятельн младш школьник обуче...  \n",
       "4   формирован пространствен представлен занят рис...  \n",
       "5   организац учебн деятельн школьник формирован п...  \n",
       "6   учебн исследовательск работ студент методик пр...  \n",
       "7   совершенствован преподаван электрорадиотехник ...  \n",
       "8   формирован логическ мышлен уча 6 8 класс обуче...  \n",
       "9   особен организац учебн эксперимент школ углубл...  \n",
       "10  развит математическ культур школьник языков ас...  \n",
       "11  обучен морфолог основ обобщен понят систематич...  \n",
       "12  теоретическ основ обучен орфограф средн школме...  \n",
       "13  лингводидактическ основ создан использован ауд...  \n",
       "14  методическ основ обучен культур русск реч школ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_kws'] = df['title_tr'] + df['keywords']\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-02 13:05:09.852 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from C:\\Users\\Ирина\\PycharmProjects\\UIR7sem\\venv\\tfidf_knn.pkl\n",
      "2020-03-02 13:05:10.4 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-03-02 13:05:10.16 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "tfidf_knn = SklearnComponent(\n",
    "    model_class=\"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "    infer_method=\"transform\",\n",
    "    save_path='./tfidf_knn.pkl',\n",
    "    load_path='./tfidf_knn.pkl',\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-02 13:05:59.313 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-03-02 13:06:01.209 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to C:\\Users\\Ирина\\PycharmProjects\\UIR7sem\\venv\\tfidf_knn.pkl\n"
     ]
    }
   ],
   "source": [
    "x = df['title_kws'].values.tolist()\n",
    "x = tuple(x)\n",
    "tfidf_knn.fit(x)\n",
    "tfidf_knn.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['title_kws'].values.tolist()\n",
    "for i in range(len(x)):\n",
    "    if isinstance(x[i],str) is False:\n",
    "        x[i]='NONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tfidf_knn(x).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-87c5ebc117eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mneigh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ball_tree'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'precomputed'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m         \"\"\"\n\u001b[1;32m--> 930\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# safely to reduce dtype induced overflows.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mis_float\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'fc'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mis_float\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_safe_accumulator_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_float\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36m_safe_accumulator_op\u001b[1;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2076\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " neigh = NearestNeighbors(n_neighbors=5,algorithm='ball_tree').fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ('математика интегральное исчисление интеграл',)\n",
    "y = tfidf_knn(y).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26058"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.kneighbors(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.24409592, 1.27462216, 1.28910011, 1.28975918, 1.2927336 ]]),\n",
       " array([[ 172, 6991, 3049, 1212, 2549]], dtype=int64))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=5).fit(X)\n",
    "text = treatment_text('колебание коэффициент демпфирования модуль упругость установка')\n",
    "y = (text,)\n",
    "y = vectorizer.transform(y)\n",
    "y =y.toarray()\n",
    "neigh.kneighbors(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.toarray()[9353]\n",
    "#  array([[9353, 8997, 9269, 9799, 8963]], dtype=int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вычислительн схем линейн программирован перемен коэффициент примененматематическ кибернетик'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_kws'][3049]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from deeppavlov.models.sklearn import SklearnComponent\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "stops.add('рис')\n",
    "stops.add('университет')\n",
    "stops.add('брянск')\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "def treatment_text(review):\n",
    "    try:\n",
    "        review_text = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", review)\n",
    "        words = review_text.lower().split()\n",
    "        words = [w for w in words if not w in stops]\n",
    "        words = [morph.parse(w)[0].normal_form for w in words]\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        words = [w for w in words if not w in stops]\n",
    "        return(' '.join(words))\n",
    "    except:\n",
    "        return review\n",
    "from google.oauth2 import service_account\n",
    "project_id = 'arctic-task-238719'\n",
    "private_key='arctic-task-238719-e6a1c5fe056b.json'\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "credentials = service_account.Credentials.from_service_account_file('./arctic-task-238719-e6a1c5fe056b.json')\n",
    "from pandas.io import gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: The default value for dialect is changing to \"standard\" in a future version of pandas-gbq. Pass in dialect=\"legacy\" to disable this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "Downloading: 100%|███████████████████| 10955/10955 [00:04<00:00, 2702.16rows/s]\n"
     ]
    }
   ],
   "source": [
    "Query = 'SELECT * FROM dataset.search_rsl_ru '\n",
    "        \n",
    "df = gbq.read_gbq(Query, project_id, credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keyword_tr'] = df['keywords'].apply(treatment_text)\n",
    "df['title_tr'] = df['title'].apply(treatment_text)\n",
    "df['title_kws'] = df['keyword_tr'] + df['title_tr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['title_kws'].values.tolist()\n",
    "for i in range(len(x)):\n",
    "    if isinstance(x[i],str) is False:\n",
    "        x[i]='NONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.24753628, 1.26874884, 1.2740085 , 1.27424685, 1.29116827]]),\n",
       " array([[ 172, 6991, 9400, 2677, 3049]], dtype=int64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=5).fit(X)\n",
    "text = treatment_text('колебание коэффициент демпфирования модуль упругость установка')\n",
    "y = (text,)\n",
    "y = vectorizer.transform(y)\n",
    "y =y.toarray()\n",
    "neigh.kneighbors(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vectorizer.pickle', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "with open('vectorizer.pickle', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "with open('knn.pickle', 'wb') as f:\n",
    "    pickle.dump(neigh, f)\n",
    "with open('knn.pickle', 'rb') as f:\n",
    "    neigh = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.pickle', 'wb') as f:\n",
    "    pickle.dump(X, f)\n",
    "with open('train.pickle', 'rb') as f:\n",
    "    X = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open('vectorizer.pickle', 'rb') as f:\n",
    "#     vectorizer = pickle.load(f)\n",
    "\n",
    "# with open('knn.pickle', 'rb') as f:\n",
    "#     neigh = pickle.load(f)\n",
    "def find_similar(text,kws,ann):\n",
    "\n",
    "    s = kws+ann\n",
    "    s = treatment_text(s)\n",
    "    s = vectorizer.transform([s])\n",
    "    s = s.toarray()\n",
    "    result = neigh.kneighbors(y)[1][0]\n",
    "    r =[]\n",
    "    for i in result:\n",
    "        r.append(df.loc[i][['author','title','keywords']].values.tolist())\n",
    "    return r\n",
    "r =find_similar('колебание коэффициент демпфирования модуль упругость установка','','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Перминова Мария Юрьевна</td>\n",
       "      <td>Алгоритмы и программный модуль получения явных...</td>\n",
       "      <td>теоретические основы информатики, физико-матем...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Фаворская Алена Владимировна</td>\n",
       "      <td>Метод исследования пространственных волновых я...</td>\n",
       "      <td>математическое моделирование, численные методы...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Финошин Александр Викторович</td>\n",
       "      <td>Адаптивное управление нелинейными колебаниями</td>\n",
       "      <td>системный анализ, управление и обработка инфор...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Глотова Людмила Сергеевна</td>\n",
       "      <td>Рассеяние энергии механических колебаний в мяг...</td>\n",
       "      <td>физика магнитных явлений</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Мартынов Анатолий Поликарпович</td>\n",
       "      <td>Вычислительные схемы линейного программировани...</td>\n",
       "      <td>математическая кибернетика</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                1  \\\n",
       "0         Перминова Мария Юрьевна   \n",
       "1    Фаворская Алена Владимировна   \n",
       "2    Финошин Александр Викторович   \n",
       "3       Глотова Людмила Сергеевна   \n",
       "4  Мартынов Анатолий Поликарпович   \n",
       "\n",
       "                                                   2  \\\n",
       "0  Алгоритмы и программный модуль получения явных...   \n",
       "1  Метод исследования пространственных волновых я...   \n",
       "2     Адаптивное управление нелинейными колебаниями    \n",
       "3  Рассеяние энергии механических колебаний в мяг...   \n",
       "4  Вычислительные схемы линейного программировани...   \n",
       "\n",
       "                                                   3  \n",
       "0  теоретические основы информатики, физико-матем...  \n",
       "1  математическое моделирование, численные методы...  \n",
       "2  системный анализ, управление и обработка инфор...  \n",
       "3                           физика магнитных явлений  \n",
       "4                         математическая кибернетика  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import pandas_gbq as gbq\n",
    "\n",
    "project_id = 'arctic-task-238719'\n",
    "private_key='arctic-task-238719-e6a1c5fe056b.json'\n",
    "credentials = service_account.Credentials.from_service_account_file('./arctic-task-238719-e6a1c5fe056b.json')\n",
    "\n",
    "pandas_gbq.context.credentials = credentials\n",
    "pandas_gbq.context.project = project_id\n",
    "\n",
    "\n",
    "def upload_user_bd(list_of_lists,username):\n",
    "    try:\n",
    "        df = pd.DataFrame(list_of_lists, columns=['authors','title','keywords'])\n",
    "        gbq.to_gbq(df,'dataset.'+username, project_id , if_exists = 'append'   )\n",
    "        return True\n",
    "    except TransportError:\n",
    "        return False\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:06,  6.54s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upload_user_bd(r,'kirill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.oauth2 import service_account\n",
    "project_id = 'arctic-task-238719'\n",
    "private_key='arctic-task-238719-e6a1c5fe056b.json'\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "credentials = service_account.Credentials.from_service_account_file('./arctic-task-238719-e6a1c5fe056b.json')\n",
    "from pandas.io import gbq\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def similar_articles_from_user_library(username,text,kws,ann):\n",
    "    try:\n",
    "        Query = 'SELECT * FROM dataset.'+username\n",
    "        df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "        df['title_kws'] = df['title']+df['keywords']\n",
    "        df['title_kws'] = df['title_kws'].apply(treatment_text)\n",
    "        x = df['title_kws'].values.tolist()\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X = vectorizer.fit_transform(x)\n",
    "        neigh = NearestNeighbors(n_neighbors=5).fit(X)\n",
    "        s = treatment_text(kws+ann)\n",
    "        s = vectorizer.transform([s]).toarray()\n",
    "        result = neigh.kneighbors(s)[1][0]\n",
    "        r =[]\n",
    "        for i in result:\n",
    "            r.append(df.loc[i][['authors','title','keywords']].values.tolist())\n",
    "        return r\n",
    "    except TransportError:\n",
    "        return False\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\lib\\site-packages\\ipykernel_launcher.py:44: FutureWarning: The default value for dialect is changing to \"standard\" in a future version of pandas-gbq. Pass in dialect=\"legacy\" to disable this warning.\n",
      "Downloading: 0rows [00:01, ?rows/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # search_articles\n",
    "# import re\n",
    "# from nltk.corpus import stopwords\n",
    "# import pymorphy2\n",
    "# from google.oauth2 import service_account\n",
    "# project_id = 'arctic-task-238719'\n",
    "# private_key='arctic-task-238719-e6a1c5fe056b.json'\n",
    "# import json\n",
    "# from google.cloud import bigquery\n",
    "# credentials = service_account.Credentials.from_service_account_file('./arctic-task-238719-e6a1c5fe056b.json')\n",
    "# from pandas.io import gbq\n",
    "# stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "# import pandas as pd\n",
    "# morph=pymorphy2.MorphAnalyzer()\n",
    "\n",
    "# def search(word='',mode='title'):\n",
    "#     word = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", word)\n",
    "#     words = word.lower().split()\n",
    "#     words = [w for w in words if not w in stops]\n",
    "#     if words=='':\n",
    "#         return \"некорректный ввод\"\n",
    "\n",
    "#     if mode=='author':\n",
    "#         Query = 'SELECT * FROM dataset.search_rsl_ru WHERE AUTHOR LIKE \\''\n",
    "#         for word in words:\n",
    "#             Query+='%{}'.format(word)\n",
    "#         Query += '%\\''\n",
    "#         df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "\n",
    "#     if mode=='title':\n",
    "#         words = [morph.parse(w)[0].normal_form for w in words]\n",
    "#         Query = 'SELECT * FROM dataset.search_rsl_ru WHERE TITLE LIKE \\''\n",
    "#         for word in words:\n",
    "#             Query+='%{}'.format(word)\n",
    "#         Query += '%\\''\n",
    "#         df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "\n",
    "#     if mode=='kws':\n",
    "#         words = [morph.parse(w)[0].normal_form for w in words]\n",
    "#         Query = 'SELECT * FROM dataset.search_rsl_ru WHERE KEYWORDS LIKE \\''\n",
    "#         for word in words:\n",
    "#             Query+='%{}'.format(word)\n",
    "#         Query +='%\\''\n",
    "#         df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "#     result = df.values.tolist()\n",
    "#     if result==[]:\n",
    "#         return False\n",
    "#     else:\n",
    "#         return result\n",
    "\n",
    "# search(mode='kws',word='колебания')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT * FROM dataset.kirill WHERE KEYWORDS LIKE '%колебан%механическ%'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\python\\lib\\site-packages\\ipykernel_launcher.py:47: FutureWarning: The default value for dialect is changing to \"standard\" in a future version of pandas-gbq. Pass in dialect=\"legacy\" to disable this warning.\n",
      "Downloading: 100%|█████████████████████████████| 3/3 [00:01<00:00,  2.65rows/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Финошин Александр Викторович', 'Адаптивное управление нелинейными колебаниями ', 'системный анализ, управление и обработка информации, физико-математические науки,механика,теоретическая механика,динамика,динамика системы точек и твердого тела,колебания механических систем,нелинейные колебания механических систем,применение эвм'], ['Финошин Александр Викторович', 'Адаптивное управление нелинейными колебаниями ', 'системный анализ, управление и обработка информации, физико-математические науки,механика,теоретическая механика,динамика,динамика системы точек и твердого тела,колебания механических систем,нелинейные колебания механических систем,применение эвм'], ['Финошин Александр Викторович', 'Адаптивное управление нелинейными колебаниями ', 'системный анализ, управление и обработка информации, физико-математические науки,механика,теоретическая механика,динамика,динамика системы точек и твердого тела,колебания механических систем,нелинейные колебания механических систем,применение эвм']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Финошин Александр Викторович',\n",
       "  'Адаптивное управление нелинейными колебаниями ',\n",
       "  'системный анализ, управление и обработка информации, физико-математические науки,механика,теоретическая механика,динамика,динамика системы точек и твердого тела,колебания механических систем,нелинейные колебания механических систем,применение эвм'],\n",
       " ['Финошин Александр Викторович',\n",
       "  'Адаптивное управление нелинейными колебаниями ',\n",
       "  'системный анализ, управление и обработка информации, физико-математические науки,механика,теоретическая механика,динамика,динамика системы точек и твердого тела,колебания механических систем,нелинейные колебания механических систем,применение эвм'],\n",
       " ['Финошин Александр Викторович',\n",
       "  'Адаптивное управление нелинейными колебаниями ',\n",
       "  'системный анализ, управление и обработка информации, физико-математические науки,механика,теоретическая механика,динамика,динамика системы точек и твердого тела,колебания механических систем,нелинейные колебания механических систем,применение эвм']]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "def search_user_library(username,q='',mode='title'):\n",
    "    try:\n",
    "        q = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", q)\n",
    "        q = q.lower()\n",
    "        words=q.split()\n",
    "        words = [w for w in words if not w in stops]\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        if words=='':\n",
    "            return \"некорректный ввод\"\n",
    "\n",
    "        if mode=='author':\n",
    "            Query = 'SELECT * FROM dataset.'+username+' WHERE AUTHORS LIKE \\''\n",
    "            for word in words:\n",
    "                Query+='%{}'.format(word)\n",
    "            Query += '%\\''\n",
    "            print(Query)\n",
    "            df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "            print(df.values.tolist())\n",
    "            if df.values.tolist()==[]:\n",
    "                Query = 'SELECT * FROM dataset.'+username+' WHERE AUTHORS LIKE \\'%{}%\\''.format(q)\n",
    "                print(Query)\n",
    "                df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "\n",
    "        if mode=='title':\n",
    "    #         words = [morph.parse(w)[0].normal_form for w in words]\n",
    "            Query = 'SELECT * FROM dataset.'+username+' WHERE TITLE LIKE \\''\n",
    "            for word in words:\n",
    "                Query+='%{}'.format(word)\n",
    "            Query += '%\\''\n",
    "            print(Query)\n",
    "            df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "            print(df.values.tolist())\n",
    "            if df.values.tolist()==[]:\n",
    "                Query = 'SELECT * FROM dataset.'+username+' WHERE TITLE LIKE \\'%{}%\\''.format(q)\n",
    "                print(Query)\n",
    "                df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "\n",
    "        if mode=='kws':\n",
    "    #         words = [morph.parse(w)[0].normal_form for w in words]\n",
    "            Query = 'SELECT * FROM dataset.'+username+' WHERE KEYWORDS LIKE \\''\n",
    "            for word in words:\n",
    "                Query+='%{}'.format(word)\n",
    "            Query += '%\\''\n",
    "            print(Query)\n",
    "            df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "            print(df.values.tolist())\n",
    "            if df.values.tolist()==[]:\n",
    "                Query = 'SELECT * FROM dataset.'+username+' WHERE KEYWORDS LIKE \\'%{}%\\''.format(q)\n",
    "                print(Query)\n",
    "                df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "        result = df.values.tolist()\n",
    "        if result==[]:\n",
    "            return 'не найдено'\n",
    "        else:\n",
    "            return result\n",
    "    except TransportError:\n",
    "        return False\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "search_user_library(username='kirill',mode='kws',q='колебания механических')\n",
    "\n",
    "# search_user_library(username='kirill',mode='title',word='физика ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'колебания механическ'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('колебания механических')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "def search(q='',mode='title'):\n",
    "    try:\n",
    "        q = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", q)\n",
    "        q = q.lower()\n",
    "        words=q.split()\n",
    "        words = [w for w in words if not w in stops]\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        if words=='':\n",
    "            return \"некорректный ввод\"\n",
    "\n",
    "        if mode=='author':\n",
    "            Query = 'SELECT * FROM dataset.search_rsl_ru WHERE AUTHORS LIKE \\''\n",
    "            for word in words:\n",
    "                Query+='%{}'.format(word)\n",
    "            Query += '%\\''\n",
    "            print(Query)\n",
    "            df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "            print(df.values.tolist())\n",
    "            if df.values.tolist()==[]:\n",
    "                Query = 'SELECT * FROM dataset.search_rsl_ru WHERE AUTHORS LIKE \\'%{}%\\''.format(q)\n",
    "                print(Query)\n",
    "                df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "\n",
    "        if mode=='title':\n",
    "    #         words = [morph.parse(w)[0].normal_form for w in words]\n",
    "            Query = 'SELECT * FROM dataset.search_rsl_ru WHERE TITLE LIKE \\''\n",
    "            for word in words:\n",
    "                Query+='%{}'.format(word)\n",
    "            Query += '%\\''\n",
    "            print(Query)\n",
    "            df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "            print(df.values.tolist())\n",
    "            if df.values.tolist()==[]:\n",
    "                Query = 'SELECT * FROM dataset.search_rsl_ru WHERE TITLE LIKE \\'%{}%\\''.format(q)\n",
    "                print(Query)\n",
    "                df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "\n",
    "        if mode=='kws':\n",
    "    #         words = [morph.parse(w)[0].normal_form for w in words]\n",
    "            Query = 'SELECT * FROM dataset.search_rsl_ru WHERE KEYWORDS LIKE \\''\n",
    "            for word in words:\n",
    "                Query+='%{}'.format(word)\n",
    "            Query += '%\\''\n",
    "            print(Query)\n",
    "            df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "            print(df.values.tolist())\n",
    "            if df.values.tolist()==[]:\n",
    "                Query = 'SELECT * FROM dataset.search_rsl_ru WHERE KEYWORDS LIKE \\'%{}%\\''.format(q)\n",
    "                print(Query)\n",
    "                df = gbq.read_gbq(Query, project_id, credentials=credentials)\n",
    "        result = df.values.tolist()\n",
    "        if result==[]:\n",
    "            return 'не найдено'\n",
    "        else:\n",
    "            return result\n",
    "    except TransportError:\n",
    "        return False\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "# search(mode='kws',q='колебания механических')\n",
    "\n",
    "# search_user_library(username='kirill',mode='title',word='физика ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
