{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader\n",
    "from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator\n",
    "from deeppavlov.models.preprocessors.str_lower import str_lower\n",
    "from deeppavlov.models.tokenizers.nltk_moses_tokenizer import NLTKMosesTokenizer\n",
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "from deeppavlov.models.sklearn import SklearnComponent\n",
    "from deeppavlov.metrics.accuracy import sets_accuracy\n",
    "import numpy as np\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "stops.add('рис')\n",
    "stops.add('университет')\n",
    "stops.add('брянск')\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "dict_stop=set(['метод','определение','условие','момент','значение','результат','критерий',\n",
    "               'работа','вариант','брянский государственный университет','научнотехнический вестник',\n",
    "              'соответствие','такой образ','весь критерий','пример','выбор','ключевое слово','период',\n",
    "              'уравнение','формула','множитель','повышение','оценка','проведение',\n",
    "              'машина','нагрузка','брянская область','точка','случай','расчет','таблица','расчёт',\n",
    "              'с показатель','град','обработка','статья','элемент','раз','применение','центр','форма'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def treatment_text(review):\n",
    "    review_text = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    words = [w for w in words if not w in stops]\n",
    "    words = [morph.parse(w)[0].normal_form for w in words]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание dataset/x-статья,y-tag(0,1)\n",
    "# dataset = [sentence_origin,sentence_tr,tag,kws,[kw_in_sentence]]\n",
    "dataset = []\n",
    "files=['./articles_2019/articles.csv','./articles_2018/articles.csv','./articles_2017/articles.csv']\n",
    "for file in files:\n",
    "    with open(file,'r', encoding='utf-8',newline='') as f:\n",
    "        reader = csv.reader(f,delimiter=',')\n",
    "        for row in reader:\n",
    "            \n",
    "            if row!=[]:\n",
    "#                 print('eeest')\n",
    "                kws=''.join(row[0])\n",
    "                text = ''.join(row[2])\n",
    "                sentences_origin = text.split('.')\n",
    "                sentences_tr = list(map(treatment_text,sentences_origin))\n",
    "                kws_tr = treatment_text(kws)  \n",
    "                kws_tr_set=set(kws_tr)\n",
    "                \n",
    "                for sentence_tr,sentence_or in zip(sentences_tr,sentences_origin):\n",
    "                    sentence_tr_set=set(sentence_tr)\n",
    "                    if len(sentence_tr)>5 and kws_tr_set.intersection(sentence_tr_set):\n",
    "                        dataset.append((sentence_or,' '.join(sentence_tr),'1',kws,' '.join(kws_tr_set.intersection(sentence_tr_set))))\n",
    "                    elif len(sentence_tr)>5:\n",
    "                        dataset.append((sentence_or,' '.join(sentence_tr),'0',kws,'None'))\n",
    "#             else:\n",
    "#                 print('pusto')\n",
    "#                 break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# прочитать dataset из csv файла\n",
    "dr = BasicClassificationDatasetReader().read(\n",
    "    data_path='./',\n",
    "    train='dataset.csv',\n",
    "    x = 'text',\n",
    "    y = 'tag',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data iterator splitting `train` field to `train` and `valid` in proportion 0.8/0.2\n",
    "train_iterator = BasicClassificationDatasetIterator(\n",
    "    data=dr,\n",
    "    field_to_split='train',  # field that will be splitted\n",
    "    split_fields=['train', 'valid'],   # fields to which the fiald above will be splitted\n",
    "    split_proportions=[0.8, 0.2],  #proportions for splitting\n",
    "    split_seed=23,  # seed for splitting dataset 23\n",
    "    seed=42)  # seed for iteration over dataset 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = NLTKMosesTokenizer()\n",
    "train_x_lower_tokenized = str_lower(tokenizer(train_iterator.get_instances(data_type='train')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize simple vocabulary to collect all appeared in the dataset classes\n",
    "classes_vocab = SimpleVocabulary(\n",
    "    save_path='.classes.dict',\n",
    "    load_path='./classes.dict')\n",
    "classes_vocab.fit((train_iterator.get_instances(data_type='train')[1]))\n",
    "classes_vocab.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also one can collect vocabulary of textual tokens appeared 2 and more times in the dataset\n",
    "token_vocab = SimpleVocabulary(\n",
    "    save_path='./tokens.dict',\n",
    "    load_path='./tokens.dict',\n",
    "    min_freq=2,\n",
    "    special_tokens=('<PAD>', '<UNK>',),\n",
    "    unk_token='<UNK>')\n",
    "token_vocab.fit(train_x_lower_tokenized)\n",
    "token_vocab.save()\n",
    "token_vocab.freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize TF-IDF vectorizer sklearn component with `transform` as infer method\n",
    "tfidf = SklearnComponent(\n",
    "    model_class=\"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "    infer_method=\"transform\",\n",
    "    save_path='./tfidf_v0.pkl',\n",
    "    load_path='./tfidf_v0.pkl',\n",
    "    mode='train')\n",
    "tfidf.fit(str_lower(train_iterator.get_instances(data_type='train')[0]))\n",
    "tfidf.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all train and valid data from iterator\n",
    "x_train, y_train = train_iterator.get_instances(data_type=\"train\")\n",
    "x_valid, y_valid = train_iterator.get_instances(data_type=\"valid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sklearn classifier, all parameters for classifier could be passed\n",
    "cls = SklearnComponent(\n",
    "    model_class=\"sklearn.linear_model:LogisticRegression\",\n",
    "    infer_method=\"predict\",\n",
    "    save_path='./logreg_v0.pkl',\n",
    "    load_path='./logreg_v0.pkl',\n",
    "    C=1,\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit sklearn classifier and save it\n",
    "cls.fit(tfidf(x_train), y_train)\n",
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(cls, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tfidf.pickle', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "project_id = 'arctic-task-238719'\n",
    "private_key='arctic-task-238719-e6a1c5fe056b.json'\n",
    "from google.cloud import bigquery\n",
    "credentials = service_account.Credentials.from_service_account_file('./arctic-task-238719-e6a1c5fe056b.json')\n",
    "from pandas.io import gbq\n",
    "import pickle\n",
    "import re\n",
    "from deeppavlov.models.sklearn import SklearnComponent\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "import pickle\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "stops.add('рис')\n",
    "stops.add('университет')\n",
    "stops.add('брянск')\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "def treatment_text(review):\n",
    "    review_text = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", review)\n",
    "    words = review_text.lower().split()\n",
    "    words = [w for w in words if not w in stops]\n",
    "    words = [morph.parse(w)[0].normal_form for w in words]\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "    words = [w for w in words if not w in stops]\n",
    "    return(' '.join(words))\n",
    "\n",
    "\n",
    "with open('dataset.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "\n",
    "# print(df['keywords'] .head(100))\n",
    "\n",
    "\n",
    "# initialize TF-IDF vectorizer sklearn component with `transform` as infer method\n",
    "# tfidf_knn = SklearnComponent(\n",
    "#     model_class=\"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "#     infer_method=\"transform\",\n",
    "#     save_path='./tfidf_knn.pkl',\n",
    "#     load_path='./tfidf_knn.pkl',\n",
    "#     mode='train')\n",
    "#\n",
    "x = df['keywords'][:-1].values.tolist()\n",
    "x = tuple(x)\n",
    "# tfidf_knn.fit(x)\n",
    "# tfidf_knn.save()\n",
    "# with open('tfidf_knn.pickle', 'wb') as f:\n",
    "#     pickle.dump(tfidf_knn, f)\n",
    "\n",
    "with open('tfidf_knn.pickle', 'rb') as f:\n",
    "    tfidf_knn = pickle.load(f)\n",
    "y = tfidf_knn(('физика частицы квант взрыв',))\n",
    "print(y)\n",
    "#\n",
    "# # from sklearn.neighbors import NearestNeighbors\n",
    "# # x_pre =tfidf_knn(x)\n",
    "#\n",
    "# # neigh = NearestNeighbors(n_neighbors=5).fit(x_pre)\n",
    "# x_pre = x_pre.toarray()\n",
    "# if x_pre[0].all()==x_pre[1000].all():\n",
    "#     print(True)\n",
    "# answer=neigh.kneighbors(y)\n",
    "# print(neigh.kneighbors(y))\n",
    "# print(len(x_pre))\n",
    "# print('x_pre[i]',x_pre[10000])\n",
    "#\n",
    "# for i in range(x_pre.shape[0]):\n",
    "#     if x_pre[i]==answer[0]:\n",
    "#         print(x[i])\n",
    "# # print(tfidf_knn.inverse_transform(y))\n",
    "\n",
    "#\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "#\n",
    "# count_vectorizer = CountVectorizer()\n",
    "# #Apply this vectorizer to text to get a sparse matrix of counts\n",
    "# count_matrix = count_vectorizer.fit_transform(df['keywords'])\n",
    "# #Get the names of the features\n",
    "# features = count_vectorizer.get_feature_names()\n",
    "# #Create a series from the sparse matrix\n",
    "# d = pd.Series(count_matrix.toarray().flatten(),index = features).sort_values(ascending=False)\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from deeppavlov.models.sklearn import SklearnComponent\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "stops.add('рис')\n",
    "stops.add('университет')\n",
    "stops.add('брянск')\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "def treatment_text(review):\n",
    "    try:\n",
    "        review_text = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", review)\n",
    "        words = review_text.lower().split()\n",
    "        words = [w for w in words if not w in stops]\n",
    "        words = [morph.parse(w)[0].normal_form for w in words]\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        words = [w for w in words if not w in stops]\n",
    "        return(' '.join(words))\n",
    "    except:\n",
    "        return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('dataset.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "df['title_tr'] = df['title'].apply(treatment_text)\n",
    "# df['title_kws']=df['title'].apply(treatment_text)+df['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>title_tr</th>\n",
       "      <th>title_kws</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Калачев Глеб Вячеславович</td>\n",
       "      <td>О мощностной сложности плоских схем</td>\n",
       "      <td>дискретн математик математическ кибернетик выч...</td>\n",
       "      <td>мощностн сложност плоск схем</td>\n",
       "      <td>мощностн сложност плоск схемдискретн математик...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Лутцева Елена Андреевна</td>\n",
       "      <td>Педагогические основы взаимосвязи урочной и вн...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>педагогическ основ взаимосвяз урочн внеурочн т...</td>\n",
       "      <td>педагогическ основ взаимосвяз урочн внеурочн т...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Малинин Алексей Николаевич</td>\n",
       "      <td>Релятивистские идеи в курсе теоретической физи...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>релятивистск иде курс теоретическ физик педвуз</td>\n",
       "      <td>релятивистск иде курс теоретическ физик педвуз...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Царева Светлана Евгеньевна</td>\n",
       "      <td>Формирование учебной деятельности младших школ...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>формирован учебн деятельн младш школьник обуче...</td>\n",
       "      <td>формирован учебн деятельн младш школьник обуче...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Лебедко Валерий Константинович</td>\n",
       "      <td>Формирование пространственных представлений на...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>формирован пространствен представлен занят рис...</td>\n",
       "      <td>формирован пространствен представлен занят рис...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Подольский Александр Иванович</td>\n",
       "      <td>Организация учебной деятельности школьников пр...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>организац учебн деятельн школьник формирован п...</td>\n",
       "      <td>организац учебн деятельн школьник формирован п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Крахоткина Валентина Кузьминична</td>\n",
       "      <td>Учебно-исследовательская работа студентов по м...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>учебн исследовательск работ студент методик пр...</td>\n",
       "      <td>учебн исследовательск работ студент методик пр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Никифорова Валентина Михайловна</td>\n",
       "      <td>Совершенствование преподавания электрорадиотех...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>совершенствован преподаван электрорадиотехник ...</td>\n",
       "      <td>совершенствован преподаван электрорадиотехник ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Абдукаримов Мамадали</td>\n",
       "      <td>Формирование логических приемов мышления у уча...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>формирован логическ мышлен уча 6 8 класс обуче...</td>\n",
       "      <td>формирован логическ мышлен уча 6 8 класс обуче...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Ильигорский Юрий Константинович</td>\n",
       "      <td>Особенности организации учебного эксперимента ...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>особен организац учебн эксперимент школ углубл...</td>\n",
       "      <td>особен организац учебн эксперимент школ углубл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Икрамов Джурабай</td>\n",
       "      <td>Развитие математической культуры школьников  Я...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>развит математическ культур школьник языков ас...</td>\n",
       "      <td>развит математическ культур школьник языков ас...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Тростенцова Лидия Александровна</td>\n",
       "      <td>Обучение морфологии на основе ее обобщенных по...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>обучен морфолог основ обобщен понят систематич...</td>\n",
       "      <td>обучен морфолог основ обобщен понят систематич...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Разумовская Маргарита Михайловна</td>\n",
       "      <td>Теоретические основы обучения орфографии в сре...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>теоретическ основ обучен орфограф средн школ</td>\n",
       "      <td>теоретическ основ обучен орфограф средн школме...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Оганесян Сергей Саядович</td>\n",
       "      <td>Лингводидактические основы создания и использо...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>лингводидактическ основ создан использован ауд...</td>\n",
       "      <td>лингводидактическ основ создан использован ауд...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Мурина Лариса Александровна</td>\n",
       "      <td>Методические основы обучения культуре русской ...</td>\n",
       "      <td>методик преподаван отрасл наук 13 00 02 теор м...</td>\n",
       "      <td>методическ основ обучен культур русск реч школ...</td>\n",
       "      <td>методическ основ обучен культур русск реч школ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              author  \\\n",
       "0          Калачев Глеб Вячеславович   \n",
       "1            Лутцева Елена Андреевна   \n",
       "2         Малинин Алексей Николаевич   \n",
       "3         Царева Светлана Евгеньевна   \n",
       "4     Лебедко Валерий Константинович   \n",
       "5      Подольский Александр Иванович   \n",
       "6   Крахоткина Валентина Кузьминична   \n",
       "7    Никифорова Валентина Михайловна   \n",
       "8               Абдукаримов Мамадали   \n",
       "9    Ильигорский Юрий Константинович   \n",
       "10                  Икрамов Джурабай   \n",
       "11   Тростенцова Лидия Александровна   \n",
       "12  Разумовская Маргарита Михайловна   \n",
       "13          Оганесян Сергей Саядович   \n",
       "14       Мурина Лариса Александровна   \n",
       "\n",
       "                                                title  \\\n",
       "0                О мощностной сложности плоских схем    \n",
       "1   Педагогические основы взаимосвязи урочной и вн...   \n",
       "2   Релятивистские идеи в курсе теоретической физи...   \n",
       "3   Формирование учебной деятельности младших школ...   \n",
       "4   Формирование пространственных представлений на...   \n",
       "5   Организация учебной деятельности школьников пр...   \n",
       "6   Учебно-исследовательская работа студентов по м...   \n",
       "7   Совершенствование преподавания электрорадиотех...   \n",
       "8   Формирование логических приемов мышления у уча...   \n",
       "9   Особенности организации учебного эксперимента ...   \n",
       "10  Развитие математической культуры школьников  Я...   \n",
       "11  Обучение морфологии на основе ее обобщенных по...   \n",
       "12  Теоретические основы обучения орфографии в сре...   \n",
       "13  Лингводидактические основы создания и использо...   \n",
       "14  Методические основы обучения культуре русской ...   \n",
       "\n",
       "                                             keywords  \\\n",
       "0   дискретн математик математическ кибернетик выч...   \n",
       "1   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "2   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "3   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "4   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "5   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "6   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "7   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "8   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "9   методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "10  методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "11  методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "12  методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "13  методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "14  методик преподаван отрасл наук 13 00 02 теор м...   \n",
       "\n",
       "                                             title_tr  \\\n",
       "0                        мощностн сложност плоск схем   \n",
       "1   педагогическ основ взаимосвяз урочн внеурочн т...   \n",
       "2      релятивистск иде курс теоретическ физик педвуз   \n",
       "3   формирован учебн деятельн младш школьник обуче...   \n",
       "4   формирован пространствен представлен занят рис...   \n",
       "5   организац учебн деятельн школьник формирован п...   \n",
       "6   учебн исследовательск работ студент методик пр...   \n",
       "7   совершенствован преподаван электрорадиотехник ...   \n",
       "8   формирован логическ мышлен уча 6 8 класс обуче...   \n",
       "9   особен организац учебн эксперимент школ углубл...   \n",
       "10  развит математическ культур школьник языков ас...   \n",
       "11  обучен морфолог основ обобщен понят систематич...   \n",
       "12       теоретическ основ обучен орфограф средн школ   \n",
       "13  лингводидактическ основ создан использован ауд...   \n",
       "14  методическ основ обучен культур русск реч школ...   \n",
       "\n",
       "                                            title_kws  \n",
       "0   мощностн сложност плоск схемдискретн математик...  \n",
       "1   педагогическ основ взаимосвяз урочн внеурочн т...  \n",
       "2   релятивистск иде курс теоретическ физик педвуз...  \n",
       "3   формирован учебн деятельн младш школьник обуче...  \n",
       "4   формирован пространствен представлен занят рис...  \n",
       "5   организац учебн деятельн школьник формирован п...  \n",
       "6   учебн исследовательск работ студент методик пр...  \n",
       "7   совершенствован преподаван электрорадиотехник ...  \n",
       "8   формирован логическ мышлен уча 6 8 класс обуче...  \n",
       "9   особен организац учебн эксперимент школ углубл...  \n",
       "10  развит математическ культур школьник языков ас...  \n",
       "11  обучен морфолог основ обобщен понят систематич...  \n",
       "12  теоретическ основ обучен орфограф средн школме...  \n",
       "13  лингводидактическ основ создан использован ауд...  \n",
       "14  методическ основ обучен культур русск реч школ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_kws'] = df['title_tr'] + df['keywords']\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-02 13:05:09.852 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 203: Loading model sklearn.feature_extraction.text:TfidfVectorizer from C:\\Users\\Ирина\\PycharmProjects\\UIR7sem\\venv\\tfidf_knn.pkl\n",
      "2020-03-02 13:05:10.4 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 210: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2020-03-02 13:05:10.16 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 216: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "tfidf_knn = SklearnComponent(\n",
    "    model_class=\"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "    infer_method=\"transform\",\n",
    "    save_path='./tfidf_knn.pkl',\n",
    "    load_path='./tfidf_knn.pkl',\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-02 13:05:59.313 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 109: Fitting model sklearn.feature_extraction.textTfidfVectorizer\n",
      "2020-03-02 13:06:01.209 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 241: Saving model to C:\\Users\\Ирина\\PycharmProjects\\UIR7sem\\venv\\tfidf_knn.pkl\n"
     ]
    }
   ],
   "source": [
    "x = df['title_kws'].values.tolist()\n",
    "x = tuple(x)\n",
    "tfidf_knn.fit(x)\n",
    "tfidf_knn.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['title_kws'].values.tolist()\n",
    "for i in range(len(x)):\n",
    "    if isinstance(x[i],str) is False:\n",
    "        x[i]='NONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tfidf_knn(x).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-87c5ebc117eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mneigh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ball_tree'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    928\u001b[0m             \u001b[1;32mor\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'precomputed'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m         \"\"\"\n\u001b[1;32m--> 930\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 542\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m# safely to reduce dtype induced overflows.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mis_float\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'fc'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mis_float\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_safe_accumulator_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mis_float\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\sklearn\\utils\\extmath.py\u001b[0m in \u001b[0;36m_safe_accumulator_op\u001b[1;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[0;32m   2074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[1;32m-> 2076\u001b[1;33m                           initial=initial)\n\u001b[0m\u001b[0;32m   2077\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\python\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " neigh = NearestNeighbors(n_neighbors=5,algorithm='ball_tree').fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ('математика интегральное исчисление интеграл',)\n",
    "y = tfidf_knn(y).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26058"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh.kneighbors(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.24409592, 1.27462216, 1.28910011, 1.28975918, 1.2927336 ]]),\n",
       " array([[ 172, 6991, 3049, 1212, 2549]], dtype=int64))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=5).fit(X)\n",
    "text = treatment_text('колебание коэффициент демпфирования модуль упругость установка')\n",
    "y = (text,)\n",
    "y = vectorizer.transform(y)\n",
    "y =y.toarray()\n",
    "neigh.kneighbors(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.toarray()[9353]\n",
    "#  array([[9353, 8997, 9269, 9799, 8963]], dtype=int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'вычислительн схем линейн программирован перемен коэффициент примененматематическ кибернетик'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title_kws'][3049]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from deeppavlov.models.sklearn import SklearnComponent\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "project_id = 'arctic-task-238719'\n",
    "private_key='arctic-task-238719-e6a1c5fe056b.json'\n",
    "import json\n",
    "from google.cloud import bigquery\n",
    "credentials = service_account.Credentials.from_service_account_file('./arctic-task-238719-e6a1c5fe056b.json')\n",
    "from pandas.io import gbq\n",
    "\n",
    "stops = set(stopwords.words(\"english\")) | set(stopwords.words(\"russian\"))\n",
    "stops.add('рис')\n",
    "stops.add('университет')\n",
    "stops.add('брянск')\n",
    "\n",
    "morph=pymorphy2.MorphAnalyzer()\n",
    "stemmer=SnowballStemmer('russian')\n",
    "\n",
    "def treatment_text(review):\n",
    "    try:\n",
    "        review_text = re.sub(\"[^а-яА-Яa-zA-Z0-9]\", \" \", review)\n",
    "        words = review_text.lower().split()\n",
    "        words = [w for w in words if not w in stops]\n",
    "        words = [morph.parse(w)[0].normal_form for w in words]\n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        words = [w for w in words if not w in stops]\n",
    "        return(' '.join(words))\n",
    "    except:\n",
    "        return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 10955/10955 [00:03<00:00, 2979.21rows/s]\n"
     ]
    }
   ],
   "source": [
    "Query = 'SELECT * FROM dataset.search_rsl_ru '\n",
    "        \n",
    "df = gbq.read_gbq(Query, project_id, credentials=credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['keyword_tr'] = df['keywords'].apply(treatment_text)\n",
    "df['title_tr'] = df['title'].apply(treatment_text)\n",
    "df['title_kws'] = df['keyword_tr'] + df['title_tr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['title_kws'].values.tolist()\n",
    "for i in range(len(x)):\n",
    "    if isinstance(x[i],str) is False:\n",
    "        x[i]='NONE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kirill/2019-2-Atom-Backend-K-Kondratenya/venv/lib/python3.6/site-packages/sklearn/neighbors/base.py:216: UserWarning: cannot use tree with sparse input: using brute force\n",
      "  warnings.warn(\"cannot use tree with sparse input: \"\n"
     ]
    }
   ],
   "source": [
    " neigh = NearestNeighbors(n_neighbors=5,algorithm='ball_tree').fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NearestNeighbors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-83051bb65233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mneigh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtreatment_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'азерная физика, физико-математические науки,физика,физика твердого тела, кристаллография,оптические свойства твердых тел, оптические свойства кристаллов (кристаллооптика),оптические свойства наноструктур твердых тел, физико-математические науки,физика,физика твердого тела'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'NearestNeighbors' is not defined"
     ]
    }
   ],
   "source": [
    "neigh = NearestNeighbors(n_neighbors=5).fit(X)\n",
    "text = treatment_text('азерная физика, физико-математические науки,физика,физика твердого тела, кристаллография,оптические свойства твердых тел, оптические свойства кристаллов (кристаллооптика),оптические свойства наноструктур твердых тел, физико-математические науки,физика,физика твердого тела')\n",
    "y = [text]\n",
    "y = vectorizer.transform(y)\n",
    "y =y.toarray()\n",
    "neigh.kneighbors(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3f64da5032b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vectorizer.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'vectorizer.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'knn.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "with open('vectorizer.pickle', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "with open('vectorizer.pickle', 'rb') as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "with open('knn.pickle', 'wb') as f:\n",
    "    pickle.dump(neigh, f)\n",
    "with open('knn.pickle', 'rb') as f:\n",
    "    neigh = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author                                  Перминова Мария Юрьевна\n",
       "title         Алгоритмы и программный модуль получения явных...\n",
       "keywords      теоретические основы информатики, физико-матем...\n",
       "keyword_tr    теоретическ основ информатик физик математичес...\n",
       "title_tr      алгоритм программн модул получен явн выражен к...\n",
       "title_kws     теоретическ основ информатик физик математичес...\n",
       "Name: 172, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
